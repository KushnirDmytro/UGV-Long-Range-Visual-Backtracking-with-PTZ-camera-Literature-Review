@article{ALARURI20165820,
  title    = {Calculating the modulation transfer function of an optical imaging system incorporating a digital camera from slanted-edge images captured under variable illumination levels: Fourier transforms application using MATLAB},
  journal  = {Optik},
  volume   = {127},
  number   = {15},
  pages    = {5820-5824},
  year     = {2016},
  issn     = {0030-4026},
  doi      = {https://doi.org/10.1016/j.ijleo.2016.04.018},
  url      = {https://www.sciencedirect.com/science/article/pii/S0030402616302911},
  author   = {Sami D. Alaruri},
  keywords = {Modulation transfer function (MTF), Line spread function (LSF), Point spread function (PSF), Optical transfer function (OTF), ImageJ, Collimator},
  abstract = {In this paper, a method for evaluating the modulation transfer function (MTF) of an optical system from the image of a slanted-edge target projected using an off-axis Newtonian collimator and recorded using a digital camera is presented. Mathematical derivation for the MTF function in terms of the line spread function employing Fourier transforms and convolution theory is provided. Furthermore, MATLAB® script for calculating the MTF plot from the captured slanted-edge images is given. As an illustration, MTF plots computed for slanted-edge images captured under 100, 500 and 1000lx illumination levels are discussed.}
}

@article{Artmann2010_ResolutionMetrology_NoiseReduction,
  author  = {Artmann, Uwe and Wueller, Dietmar},
  year    = {2010},
  month   = {01},
  pages   = {},
  title   = {Differences of digital camera resolution metrology to describe noise reduction artifacts},
  volume  = {7529},
  journal = {Proceedings of SPIE - The International Society for Optical Engineering},
  doi     = {10.1117/12.838743}
}

@inproceedings{AutonomousChallenges_2016,
  title     = {Active planning based extrinsic calibration of exteroceptive sensors in unknown environments},
  url       = {http://dx.doi.org/10.1109/IROS.2016.7759389},
  doi       = {10.1109/iros.2016.7759389},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  publisher = {IEEE},
  author    = {Murali,  Varun and Nieto,  Carlos and Choudhary,  Siddharth and Christensen,  Henrik I.},
  year      = {2016},
  month     = oct,
  pages     = {2498–2505}
}
@article{Ayaz2017SurveyOZ,
  title   = {Survey on zoom-lens calibration methods and techniques},
  author  = {Shirazi Muhammad Ayaz and Min Young Kim and Jaechan Park},
  journal = {Machine Vision and Applications},
  year    = {2017},
  volume  = {28},
  pages   = {803-818},
  url     = {https://api.semanticscholar.org/CorpusID:37050176}
}


@inproceedings{BeyoundWhite_2015,
  author    = {Cheng, Dongliang and Price, Brian and Cohen, Scott and Brown, Michael S.},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Beyond White: Ground Truth Colors for Color Constancy Correction},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {298-306},
  abstract  = {A limitation in color constancy research is the inability to establish ground truth colors for evaluating corrected images. Many existing datasets contain images of scenes with a color chart included, however, only the chart's neutral colors (grayscale patches) are used to provide the ground truth for illumination estimation and correction. This is because the corrected neutral colors are known to lie along the achromatic line in the camera's color space (i.e. R=G=B), the correct RGB values of the other color patches are not known. As a result, most methods estimate a 3*3 diagonal matrix that ensures only the neutral colors are correct. In this paper, we describe how to overcome this limitation. Specifically, we show that under certain illuminations, a diagonal 3*3 matrix is capable of correcting not only neutral colors, but all the colors in a scene. This finding allows us to find the ground truth RGB values for the color chart in the camera's color space. We show how to use this information to correct all the images in existing datasets to have correct colors. Working from these new color corrected datasets, we describe how to modify existing color constancy algorithms to perform better image correction.},
  keywords  = {Image color analysis;Lighting;Cameras;Transforms;Sensitivity;Estimation;Computational modeling},
  doi       = {10.1109/ICCV.2015.42},
  issn      = {2380-7504},
  month     = {Dec}
}


@incollection{Boreman2021-mp,
  title     = {Noise-target measurement of {MTF}},
  booktitle = {Modulation Transfer Function in Optical and {Electro-Optical}
               Systems, Second Edition},
  author    = {Boreman, Glenn D},
  publisher = {SPIE},
  month     = jan,
  year      = 2021
}
@article{Bowman2020FlatField,
  title     = {Flat-Field and Colour Correction for the Raspberry Pi Camera Module},
  volume    = {4},
  issn      = {2514-1708},
  url       = {http://dx.doi.org/10.5334/joh.20},
  doi       = {10.5334/joh.20},
  number    = {1},
  journal   = {Journal of Open Hardware},
  publisher = {University of Western Ontario,  Western Libraries},
  author    = {Bowman,  Richard W. and Vodenicharski,  Boyko and Collins,  Joel T. and Stirling,  Julian},
  year      = {2020}
}



@article{Burns2007,
  title     = {Ten Tips for Maintaining Digital Image Quality},
  volume    = {4},
  issn      = {2161-8798},
  url       = {http://dx.doi.org/10.2352/issn.2168-3204.2007.4.1.art00005},
  doi       = {10.2352/issn.2168-3204.2007.4.1.art00005},
  number    = {1},
  journal   = {Archiving Conference},
  publisher = {Society for Imaging Science & Technology},
  author    = {Burns,  Peter D. and Williams,  Don},
  year      = {2007},
  month     = jan,
  pages     = {16–22}
}


@inbook{Burns2014_ImagingHandbook,
  author    = {Burns, Peter D.},
  publisher = {John Wiley & Sons, Ltd},
  isbn      = {9781118798706},
  title     = {Image Quality Concepts},
  booktitle = {Handbook of Digital Imaging},
  chapter   = {},
  pages     = {1-47},
  doi       = {https://doi.org/10.1002/9781118798706.hdi004},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118798706.hdi004},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118798706.hdi004},
  year      = {2014},
  keywords  = {image quality, imaging performance, imaging standards, MTF, noise-power spectrum, image quality assessment, IQA, JND},
  abstract  = {Abstract We present several concepts useful in the specification, evaluation, and control of image quality for digital capture. We start by introducing the terminology and application of image quality models. The value of these models, however, lies in the ability to connect them to characteristics of images or imaging systems. Translating image quality into technology selection and design can be done when we have a physical understanding of design and performance. We describe methods for the objective measurement of several key imaging characteristics. They are then related to current practice by reference to several established imaging performance standards. These system-based methods are often complemented by applying human vision models. Vision models are also part a larger set of techniques collectively known as Image Quality Assessment (IQA). IQA is based on extracting measures directly from digital images, particularly useful for processing paths which adapt to image content.}
}




@inproceedings{BurnsWilliams2001_DiagnosticsByMTF,
  author  = {Williams, Don and Burns, Peter},
  year    = {2001},
  month   = {01},
  pages   = {227-232},
  title   = {Diagnostics for Digital Capture Using MTF.},
  volume  = {4},
  journal = {Proc IS&T PICS Conference}
}
@inproceedings{ColorReproductionAccuracy_2018,
  author = {Can, Hakki and Brown, Michael},
  year   = {2018},
  month  = {06},
  pages  = {6440-6449},
  title  = {Improving Color Reproduction Accuracy on Cameras},
  doi    = {10.1109/CVPR.2018.00674}
}
@article{Determine_Response_What_is_knowable_2003,
  author  = {Grossberg, Michael and Nayar, Shree},
  year    = {2003},
  month   = {12},
  pages   = {1455- 1467},
  title   = {Determining the camera response from images: What is knowable?},
  volume  = {25},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  doi     = {10.1109/TPAMI.2003.1240119}
}
@article{FastAndAccurate_autoFocus_algorithm,
  author  = {xuedian, zhang and Liu, Zhaoqing and Jiang, Minshan and Chang, Min},
  year    = {2014},
  month   = {12},
  pages   = {},
  title   = {Fast and accurate auto-focusing algorithm based on the combination of depth from focus and improved depth from defocus},
  volume  = {22},
  journal = {Optics Express},
  doi     = {10.1364/OE.22.031237}
}
@misc{FlatFieldColorCorrection2020,
  year      = {2020},
  title     = {Dataset for "Flat-field and colour correction for the Raspberry Pi camera module"},
  note      = {-   `analysis` contains the data analysis code.
               -   `data` contains the images that we used for the graphs in the manuscript.
               -   `neopixel\_driver` is the arduino firmware.
               -   `image\_acquisition` includes the Python code that acquired the images and controlled the neopixel.
               -   `calibration\_jig` contains the printable files, source OpenSCAD files, and assembly instructions for the calibration jig.
               -   `colour\_test\_sheet` contains source Inkscape SVG files and PDF renders of the test target used in the experiments.
               -   `manuscript` contains the source files for the manuscript.},
  publisher = {University of Bath},
  month     = {April},
  url       = {https://researchdata.bath.ac.uk/764/},
  abstract  = {This repository contains the hardware (OpenSCAD/STL files) and build instructions, software (Python scripts and Arduino firmware), data analysis (iPython notebook), and manuscript describing how to calibrate the colour response of a Raspberry Pi camera module.  It also includes the calibration images acquired during the preparation of the work.},
  author    = {Bowman, Richard and Stirling, Julian and Collins, Joel and Vodenicharski, Boyko}
}


@article{FoundationPiCamera2.1,
  author    = {Mary A. Pagnutti and Robert E. Ryan and George J. Cazenavette V and Maxwell J. Gold and Ryan Harlan and Edward Leggett and James F. Pagnutti},
  title     = {{Laying the foundation to use Raspberry Pi 3 V2 camera module imagery for scientific and engineering purposes}},
  volume    = {26},
  journal   = {Journal of Electronic Imaging},
  number    = {1},
  publisher = {SPIE},
  pages     = {013014},
  abstract  = {A comprehensive radiometric characterization of raw-data format imagery acquired with the Raspberry Pi 3 and V2.1 camera module is presented. The Raspberry Pi is a high-performance single-board computer designed to educate and solve real-world problems. This small computer supports a camera module that uses a Sony IMX219 8 megapixel CMOS sensor. This paper shows that scientific and engineering-grade imagery can be produced with the Raspberry Pi 3 and its V2.1 camera module. Raw imagery is shown to be linear with exposure and gain (ISO), which is essential for scientific and engineering applications. Dark frame, noise, and exposure stability assessments along with flat fielding results, spectral response measurements, and absolute radiometric calibration results are described. This low-cost imaging sensor, when calibrated to produce scientific quality data, can be used in computer vision, biophotonics, remote sensing, astronomy, high dynamic range imaging, and security applications, to name a few.},
  keywords  = {imaging, radiometric calibration, Raspberry Pi camera, Cameras, Calibration, Sensors, Image sensors, Optical spheres, Integrating spheres, RGB color model, Data acquisition, Data modeling, Interference (communication)},
  year      = {2017},
  doi       = {10.1117/1.JEI.26.1.013014},
  url       = {https://doi.org/10.1117/1.JEI.26.1.013014}
}

@article{GNC_2020,
  title     = {Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection},
  volume    = {5},
  issn      = {2377-3774},
  url       = {http://dx.doi.org/10.1109/LRA.2020.2965893},
  doi       = {10.1109/lra.2020.2965893},
  number    = {2},
  journal   = {IEEE Robotics and Automation Letters},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Yang, Heng and Antonante, Pasquale and Tzoumas, Vasileios and Carlone, Luca},
  year      = {2020},
  month     = apr,
  pages     = {1127–1134}
}
@book{HandbookOpticalSystems2012,
  title     = {Handbook of Optical Systems: Volume 5: Metrology of Optical Components and Systems},
  isbn      = {9783527699230},
  url       = {http://dx.doi.org/10.1002/9783527699230},
  doi       = {10.1002/9783527699230},
  publisher = {Wiley},
  year      = {2012},
  month     = apr
}



@article{Hayes2021,
  title     = {Pre-Flight Calibration of the Mars 2020 Rover Mastcam Zoom (Mastcam-Z) Multispectral,  Stereoscopic Imager},
  volume    = {217},
  issn      = {1572-9672},
  url       = {http://dx.doi.org/10.1007/s11214-021-00795-x},
  doi       = {10.1007/s11214-021-00795-x},
  number    = {2},
  journal   = {Space Science Reviews},
  publisher = {Springer Science and Business Media LLC},
  author    = {Hayes,  Alexander G. and Corlies,  P. and Tate,  C. and Barrington,  M. and Bell,  J. F. and Maki,  J. N. and Caplinger,  M. and Ravine,  M. and Kinch,  K. M. and Herkenhoff,  K. and Horgan,  B. and Johnson,  J. and Lemmon,  M. and Paar,  G. and Rice,  M. S. and Jensen,  E. and Kubacki,  T. M. and Cloutis,  E. and Deen,  R. and Ehlmann,  B. L. and Lakdawalla,  E. and Sullivan,  R. and Winhold,  A. and Parkinson,  A. and Bailey,  Z. and van Beek,  J. and Caballo-Perucha,  P. and Cisneros,  E. and Dixon,  D. and Donaldson,  C. and Jensen,  O. B. and Kuik,  J. and Lapo,  K. and Magee,  A. and Merusi,  M. and Mollerup,  J. and Scudder,  N. and Seeger,  C. and Stanish,  E. and Starr,  M. and Thompson,  M. and Turenne,  N. and Winchell,  K.},
  year      = {2021},
  month     = feb
}



@inproceedings{Herrmann_2020_LearningAutofocus,
  author    = {Herrmann, Charles and Bowen, Richard Strong and Wadhwa, Neal and Garg, Rahul and He, Qiurui and Barron, Jonathan T. and Zabih, Ramin},
  title     = {Learning to Autofocus},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2020}
}


@misc{ImatestColorMatrix,
  author = {{Imatest LLC}},
  title  = {Color Matrix - Overview and Calibration},
  year   = {2024},
  url    = {https://www.imatest.com/docs/colormatrix/},
  note   = {Accessed: 2024-11-15}
}


@article{Kautsky2002_ImageFocus,
  author  = {Kautsky, Jaroslav and Flusser, Jan and Zitová, Barbara and Šimberová, Stanislava},
  year    = {2002},
  month   = {12},
  pages   = {1785-1794},
  title   = {A new wavelet-based measure of image focus},
  volume  = {23},
  journal = {Pattern Recognition Letters},
  doi     = {10.1016/S0167-8655(02)00152-6}
}


@article{Koren2020,
  title     = {Measuring camera Shannon Information Capacity with a Siemens Star Image},
  volume    = {32},
  issn      = {2470-1173},
  url       = {http://dx.doi.org/10.2352/ISSN.2470-1173.2020.9.IQSP-347},
  doi       = {10.2352/issn.2470-1173.2020.9.iqsp-347},
  number    = {9},
  journal   = {Electronic Imaging},
  publisher = {Society for Imaging Science & Technology},
  author    = {Koren,  Norman L.},
  year      = {2020},
  month     = jan,
  pages     = {347--1--347–10}
}
@article{Krotkov1988,
  title     = {Focusing},
  volume    = {1},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/BF00127822},
  doi       = {10.1007/bf00127822},
  number    = {3},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Krotkov,  Eric},
  year      = {1988},
  month     = oct,
  pages     = {223–237}
}


@article{Lee2009,
  title     = {Reduced Energy-Ratio Measure for Robust Autofocusing in Digital Camera},
  volume    = {16},
  issn      = {1070-9908},
  url       = {http://dx.doi.org/10.1109/LSP.2008.2008938},
  doi       = {10.1109/lsp.2008.2008938},
  number    = {2},
  journal   = {IEEE Signal Processing Letters},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Lee,  Sang-Yong and Yoo,  Jae-Tack and Kumar,  Yogendera and Kim,  Soo-Won},
  year      = {2009},
  month     = feb,
  pages     = {133–136}
}

@article{Li2019,
  title     = {Laboratory Geometric Calibration of Digital Aerial Camera based on Focused Collimator-Array},
  volume    = {237},
  issn      = {1755-1315},
  url       = {http://dx.doi.org/10.1088/1755-1315/237/3/032007},
  doi       = {10.1088/1755-1315/237/3/032007},
  journal   = {IOP Conference Series: Earth and Environmental Science},
  publisher = {IOP Publishing},
  author    = {Li,  Ying and Liu,  Fengzhu},
  year      = {2019},
  month     = mar,
  pages     = {032007}
}

@misc{liao2024deeplearningcameracalibration,
  title         = {Deep Learning for Camera Calibration and Beyond: A Survey},
  author        = {Kang Liao and Lang Nie and Shujuan Huang and Chunyu Lin and Jing Zhang and Yao Zhao and Moncef Gabbouj and Dacheng Tao},
  year          = {2024},
  eprint        = {2303.10559},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2303.10559}
}
@article{Liu:22,
  author    = {Jixiao Liu and Jupu Yang and Jingchen Li and Jialin Du and Siyang Yu and Fanxing Li and Jian Wang and Xi Zhang and Si Sun and Wei Yan},
  journal   = {Opt. Express},
  keywords  = {Camera calibration; Deep learning; Imaging systems; Neural networks; Optical imaging; Three dimensional reconstruction},
  number    = {13},
  pages     = {23511--23530},
  publisher = {Optica Publishing Group},
  title     = {High-precision calibration to zoom lens of optical measurement machine based on FNN},
  volume    = {30},
  month     = {Jun},
  year      = {2022},
  url       = {https://opg.optica.org/oe/abstract.cfm?URI=oe-30-13-23511},
  doi       = {10.1364/OE.459771},
  abstract  = {We proposed a calibration method for high-precision zoom lenses of optical measurement machines based on Fully Connected Neural Network (FNN), using a 5-layer neural network instead of a camera calibration model, to achieve continuous calibration of zoom lenses at any zoom setting by calibrating typical zooms. From the experimental verification, the average calibration error of this method is 9.83\&\#x00D7;10\&\#x2212;4mm and the average measurement error at any zoom setting is 0.01317mm. The overall calibration precision is better than that of Zhang\&\#x0027;s calibration method and can meet the application requirements of a high-precision optical measurement machine. The method proposed in this paper provided a new solution and a new idea for the calibration of zoom lenses, which can be widely used in the fields of precision parts inspection and machine-vision measurement.}
}
@inproceedings{Lochman2021_MinSolvers,
  title     = {Minimal Solvers for Single-View Lens-Distorted Camera Auto-Calibration},
  url       = {http://dx.doi.org/10.1109/WACV48630.2021.00293},
  doi       = {10.1109/wacv48630.2021.00293},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  publisher = {IEEE},
  author    = {Lochman,  Yaroslava and Dobosevych,  Oles and Hryniv,  Rostyslav and Pritts,  James},
  year      = {2021},
  month     = jan,
  pages     = {2886–2895}
}


@article{Loebich2007_SiemensStar,
  author  = {Loebich, Christian and Wueller, Dietmar and Klingen, Bruno and Jaeger, Anke},
  year    = {2007},
  month   = {02},
  pages   = {},
  title   = {Digital camera resolution measurement using sinusoidal Siemens stars},
  volume  = {6502},
  journal = {SPIE},
  doi     = {10.1117/12.703817}
}


@article{Meiner2018,
  title     = {TOWARDS STANDARDIZED EVALUATION OF IMAGE QUALITY FOR AIRBORNE CAMERA SYSTEMS},
  volume    = {XLII–1},
  issn      = {2194-9034},
  url       = {http://dx.doi.org/10.5194/isprs-archives-XLII-1-295-2018},
  doi       = {10.5194/isprs-archives-xlii-1-295-2018},
  journal   = {The International Archives of the Photogrammetry,  Remote Sensing and Spatial Information Sciences},
  publisher = {Copernicus GmbH},
  author    = {Meißner,  H. and Cramer,  M. and Reulke,  R.},
  year      = {2018},
  month     = sep,
  pages     = {295–300}
}


@inproceedings{Mitsunaga_Radiomentric_selfcalib_1999,
  series     = {CVPR-99},
  title      = {Radiometric self calibration},
  url        = {http://dx.doi.org/10.1109/CVPR.1999.786966},
  doi        = {10.1109/cvpr.1999.786966},
  booktitle  = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
  publisher  = {IEEE Comput. Soc},
  author     = {Mitsunaga,  T. and Nayar,  S.K.},
  pages      = {374–380},
  collection = {CVPR-99}
}


@article{ModelingSpaceOfResponseFunctions_2004,
  author   = {Grossberg, M.D. and Nayar, S.K.},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Modeling the space of camera response functions},
  year     = {2004},
  volume   = {26},
  number   = {10},
  pages    = {1272-1282},
  abstract = {Many vision applications require precise measurement of scene radiance. The function relating scene radiance to image intensity of an imaging system is called the camera response. We analyze the properties that all camera responses share. This allows us to find the constraints that any response function must satisfy. These constraints determine the theoretical space of all possible camera responses. We have collected a diverse database of real-world camera response functions (DoRF). Using this database, we show that real-world responses occupy a small part of the theoretical space of all possible responses. We combine the constraints from our theoretical space with the data from DoRF to create a low-parameter empirical model of response (EMoR). This response model allows us to accurately interpolate the complete response function of a camera from a small number of measurements obtained using a standard chart. We also show that the model can be used to accurately estimate the camera response from images of an arbitrary scene taken using different exposures. The DoRF database and the EMoR model can be downloaded at http://www.cs.columbia.edu/CAVE.},
  keywords = {Layout;Optical imaging;Shape measurement;Digital cameras;Constraint theory;Image databases;Photometry;Geometry;Reflectivity;Optical films;Index Terms- Radiometric response function;camera response function;calibration;real-world response curves;empirical modeling;high-dynamic range;recovery of radiometry;nonlinear response;gamma correction;photometry;sensor modeling.},
  doi      = {10.1109/TPAMI.2004.88},
  issn     = {1939-3539},
  month    = {Oct}
}

@inproceedings{ModellingAndCalibration_Willson_1994,
  author       = {Reg G. Willson},
  title        = {{Modeling and calibration of automated zoom lenses}},
  volume       = {2350},
  booktitle    = {Videometrics III},
  editor       = {Sabry F. El-Hakim},
  organization = {International Society for Optics and Photonics},
  publisher    = {SPIE},
  pages        = {170 -- 186},
  abstract     = {Camera systems with automated zoom lenses are inherently more useful than those with fixed- parameter lenses. Variable-parameter lenses enable us to produce better images by matching the camera's sensing characteristics to the conditions in a scene. They also allow us to make measurements by noting how the scene's image changes as the lens settings are varied. The reason variable-parameter lenses are not more commonly used in machine vision is that they are difficult to model for continuous range of lens settings. In this paper we present a methodology for producing accurate camera models for systems with automated, variable- parameter lenses. To demonstrate our methodology's effectiveness we applied it to produce an `adjustable,' perspective-projection camera model based on Tsai's fixed camera model. Our model was calibrated and tested on an automated zoom lens where it operated across continuous ranges of focus and zoom with an average error of less than 0.11 pixels between the predicted and measured positions of features in the image plane},
  year         = {1994},
  doi          = {10.1117/12.189130},
  url          = {https://doi.org/10.1117/12.189130}
}
@inproceedings{MultiveiwGeom1DRadialCams_2005,
  author  = {Thirthala, S. and Pollefeys, Marc},
  year    = {2005},
  month   = {11},
  pages   = {1539- 1546 Vol. 2},
  title   = {Multi-view geometry of ID radial cameras and its application to omnidirectional camera calibration},
  volume  = {2},
  isbn    = {0-7695-2334-X},
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  doi     = {10.1109/ICCV.2005.158}
}
@inproceedings{NonlinearCameraResponseFunctio,
  author  = {Kim, Sunyeong and Tai, Yu-Wing and Kim, Seon and Brown, M.S. and Matsushita, Yasuyuki},
  year    = {2012},
  month   = {06},
  pages   = {25-32},
  title   = {Nonlinear Camera Response Functions and Image Deblurring},
  isbn    = {978-1-4673-1226-4},
  journal = {Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  doi     = {10.1109/CVPR.2012.6247654}
}


@article{Nugent2010,
  title     = {Measuring the modulation transfer function of an imaging spectrometer with rooflines of opportunity},
  volume    = {49},
  issn      = {0091-3286},
  url       = {http://dx.doi.org/10.1117/1.3497051},
  doi       = {10.1117/1.3497051},
  number    = {10},
  journal   = {Optical Engineering},
  publisher = {SPIE-Intl Soc Optical Eng},
  author    = {Nugent,  Paul W.},
  year      = {2010},
  month     = oct,
  pages     = {103201}
}


@article{OpenHardware,
  author   = {Oellermann, Michael and Jolles, Jolle W and Ortiz, Diego and Seabra, Rui and Wenzel, Tobias and Wilson, Hannah and Tanner, Richelle L},
  title    = {Open Hardware in Science: The Benefits of Open Electronics},
  journal  = {Integrative and Comparative Biology},
  volume   = {62},
  number   = {4},
  pages    = {1061-1075},
  year     = {2022},
  month    = {05},
  abstract = {Openly shared low-cost electronic hardware applications, known as open electronics, have sparked a new open-source movement, with much untapped potential to advance scientific research. Initially designed to appeal to electronic hobbyists, open electronics have formed a global “maker” community and are increasingly used in science and industry. In this perspective article, we review the current costs and benefits of open electronics for use in scientific research ranging from the experimental to the theoretical sciences. We discuss how user-made electronic applications can help (I) individual researchers, by increasing the customization, efficiency, and scalability of experiments, while improving data quantity and quality; (II) scientific institutions, by improving access to customizable high-end technologies, sustainability, visibility, and interdisciplinary collaboration potential; and (III) the scientific community, by improving transparency and reproducibility, helping decouple research capacity from funding, increasing innovation, and improving collaboration potential among researchers and the public. We further discuss how current barriers like poor awareness, knowledge access, and time investments can be resolved by increased documentation and collaboration, and provide guidelines for academics to enter this emerging field. We highlight that open electronics are a promising and powerful tool to help scientific research to become more innovative and reproducible and offer a key practical solution to improve democratic access to science.},
  issn     = {1540-7063},
  doi      = {10.1093/icb/icac043},
  url      = {https://doi.org/10.1093/icb/icac043},
  eprint   = {https://academic.oup.com/icb/article-pdf/62/4/1061/46682024/icac043.pdf}
}
@article{OpenHSI,
  author         = {Mao, Yiwei and Betters, Christopher H. and Evans, Bradley and Artlett, Christopher P. and Leon-Saval, Sergio G. and Garske, Samuel and Cairns, Iver H. and Cocks, Terry and Winter, Robert and Dell, Timothy},
  title          = {OpenHSI: A Complete Open-Source Hyperspectral Imaging Solution for Everyone},
  journal        = {Remote Sensing},
  volume         = {14},
  year           = {2022},
  number         = {9},
  article-number = {2244},
  url            = {https://www.mdpi.com/2072-4292/14/9/2244},
  issn           = {2072-4292},
  abstract       = {OpenHSI is an initiative to lower the barriers of entry and bring compact pushbroom hyperspectral imaging spectrometers to a wider audience. We present an open-source optical design that can be replicated with readily available commercial-off-the-shelf components, and an open-source software platform openhsi that simplifies the process of capturing calibrated hyperspectral datacubes. Some of the features that the software stack provides include: an ISO 19115-2 metadata editor, wavelength calibration, a fast smile correction method, radiance conversion, atmospheric correction using 6SV (an open-source radiative transfer code), and empirical line calibration. A pipeline was developed to customise the desired processing and make openhsi practical for real-time use. We used the OpenHSI optical design and software stack successfully in the field and verified the performance using calibration tarpaulins. By providing all the tools needed to collect documented hyperspectral datasets, our work empowers practitioners who may not have the financial or technical capability to operate commercial hyperspectral imagers, and opens the door for applications in new problem domains.},
  doi            = {10.3390/rs14092244}
}
@article{OptimalFocusMeasure_Subbarao_1998,
  author   = {Subbarao, M. and Tyan, J.-K.},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Selecting the optimal focus measure for autofocusing and depth-from-focus},
  year     = {1998},
  volume   = {20},
  number   = {8},
  pages    = {864-870},
  abstract = {A method is described for selecting the optimal focus measure with respect to gray-level noise from a given set of focus measures in passive autofocusing and depth-from-focus applications. The method is based on two new metrics that have been defined for estimating the noise-sensitivity of different focus measures. The first metric-the autofocusing uncertainty measure (AUM)-is useful in understanding the relation between gray-level noise and the resulting error in lens position for autofocusing. The second metric-autofocusing root-mean-square error (ARMS error)-is an improved metric closely related to AUM. AUM and ARMS error metrics are based on a theoretical noise sensitivity analysis of focus measures, and they are related by a monotonic expression. The theoretical results are validated by actual and simulation experiments. For a given camera, the optimally accurate focus measure may change from one object to the other depending on their focused images. Therefore, selecting the optimal focus measure from a given set involves computing all focus measures in the set.},
  keywords = {Focusing;Lenses;Noise measurement;Position measurement;Arm;Measurement uncertainty;Digital cameras;Sensitivity analysis;Computational modeling;Image analysis},
  doi      = {10.1109/34.709612},
  issn     = {1939-3539},
  month    = {Aug}
}
 @article{ParameterFreeRadialDistortion_2007,
  author   = {Hartley, Richard and Kang, Sing Bing},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Parameter-Free Radial Distortion Correction with Center of Distortion Estimation},
  year     = {2007},
  volume   = {29},
  number   = {8},
  pages    = {1309-1321},
  abstract = {We propose a method of simultaneously calibrating the radial distortion function of a camera and the other internal calibration parameters. The method relies on the use of a planar (or, alternatively, nonplanar) calibration grid which is captured in several images. In this way, the determination of the radial distortion is an easy add-on to the popular calibration method proposed. The method is entirely noniterative and, hence, is extremely rapid and immune to the problem of local minima. Our method determines the radial distortion in a parameter-free way, not relying on any particular radial distortion model. This makes it applicable to a large range of cameras from narrow-angle to fish-eye lenses. The method also computes the center of radial distortion, which, we argue, is important in obtaining optimal results. Experiments show that this point may be significantly displaced from the center of the image or the principal point of the camera.},
  keywords = {Cameras;Lenses;Calibration;Polynomials;Iterative methods;Convergence;Cost function;Optical distortion;Transmission line matrix methods;Image analysis;Radial distortion;camera calibration;fundamental matrix.},
  doi      = {10.1109/TPAMI.2007.1147},
  issn     = {1939-3539},
  month    = {Aug}
}

@article{PERTUZ20131415,
  title    = {Analysis of focus measure operators for shape-from-focus},
  journal  = {Pattern Recognition},
  volume   = {46},
  number   = {5},
  pages    = {1415-1432},
  year     = {2013},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2012.11.011},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320312004736},
  author   = {Said Pertuz and Domenec Puig and Miguel Angel Garcia},
  keywords = {Focus measure, Autofocus, Shape from focus, Defocus model},
  abstract = {Shape-from-focus (SFF) has widely been studied in computer vision as a passive depth recovery and 3D reconstruction method. One of the main stages in SFF is the computation of the focus level for every pixel of an image by means of a focus measure operator. In this work, a methodology to compare the performance of different focus measure operators for shape-from-focus is presented and applied. The selected operators have been chosen from an extensive review of the state-of-the-art. The performance of the different operators has been assessed through experiments carried out under different conditions, such as image noise level, contrast, saturation and window size. Such performance is discussed in terms of the working principles of the analyzed operators.}
}

@manual{picamera2_manual,
  title  = {Picamera2 Manual},
  author = {Raspberry Pi Foundation},
  year   = {2023},
  note   = {User manual for the Picamera2 library, which supports advanced features of the Raspberry Pi Camera, including the HQ Camera. Useful for implementation in Python-based applications.},
  url    = {https://datasheets.raspberrypi.com/camera/picamera2-manual.pdf}
}
@inbook{PlenopticZoom_2019,
  author = {Monteiro, Nuno and Gaspar, José},
  year   = {2019},
  month  = {09},
  pages  = {309-321},
  title  = {Standard Plenoptic Camera Calibration for a Range of Zoom and Focus Levels},
  isbn   = {978-3-030-31320-3},
  doi    = {10.1007/978-3-030-31321-0_27}
}
@article{Pritts_MinSolvers_2021,
  title     = {Minimal Solvers for Rectifying From Radially-Distorted Conjugate Translations},
  volume    = {43},
  issn      = {1939-3539},
  url       = {http://dx.doi.org/10.1109/TPAMI.2020.2992261},
  doi       = {10.1109/tpami.2020.2992261},
  number    = {11},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Pritts, James and Kukelova, Zuzana and Larsson, Viktor and Lochman, Yaroslava and Chum, Ondrej},
  year      = {2021},
  month     = nov,
  pages     = {3931–3948}
}


@article{Pritts_MinSolvers_Scaled_2020,
  title     = {Minimal Solvers for Rectifying from Radially-Distorted Scales and Change of Scales},
  volume    = {128},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/s11263-019-01216-x},
  doi       = {10.1007/s11263-019-01216-x},
  number    = {4},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Pritts, James and Kukelova, Zuzana and Larsson, Viktor and Lochman, Yaroslava and Chum, Ondřej},
  year      = {2020},
  month     = mar,
  pages     = {950–968}
}


@article{Pritts_multi_camera_systems_2022,
  author   = {Dexheimer, Eric and Peluse, Patrick and Chen, Jianhui and Pritts, James and Kaess, Michael},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Information-Theoretic Online Multi-Camera Extrinsic Calibration},
  year     = {2022},
  volume   = {7},
  number   = {2},
  pages    = {4757-4764},
  abstract = {Calibration of multi-camera systems is essential for lifelong use of vision-based headsets and autonomous robots. In this work, we present an information-based framework for online extrinsic calibration of multi-camera systems. While previous work largely focuses on monocular, stereo, or strictly non-overlapping field-of-view (FoV) setups, we allow arbitrary configurations while also exploiting overlapping pairwise FoV when possible. In order to efficiently solve for the extrinsic calibration parameters, which increase linearly with the number of cameras, we propose a novel entropy-based keyframe measure and bound the backend optimization complexity by selecting informative motion segments that minimize the maximum entropy across all extrinsic parameter partitions. We validate the pipeline on three distinct platforms to demonstrate the generality of the method for resolving the extrinsics and performing downstream tasks. Our code is available at https://github.com/edexheim/info_ext_calib.},
  keywords = {Calibration;Cameras;Entropy;Optimization;Current measurement;Motion segmentation;Uncertainty;SLAM;calibration and identification},
  doi      = {10.1109/LRA.2022.3145061},
  issn     = {2377-3766},
  month    = {April}
}


@article{Pritts_Multicamera_Online_Pose,
  author  = {Dexheimer, Eric and Peluse, Patrick and Chen, Jianhui and Pritts, James and Kaess, Michael},
  year    = {2022},
  month   = {04},
  pages   = {1-1},
  title   = {Information-Theoretic Online Multi-Camera Extrinsic Calibration},
  volume  = {7},
  journal = {IEEE Robotics and Automation Letters},
  doi     = {10.1109/LRA.2022.3145061}
}


@misc{pritts2018rectificationradiallydistortedscales,
  title         = {Rectification from Radially-Distorted Scales},
  author        = {James Pritts and Zuzana Kukelova and Viktor Larsson and Ondrej Chum},
  year          = {2018},
  eprint        = {1807.06110},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1807.06110}
}
@article{RadiallyDistorted_conjugate_trans_2018,
  author     = {James Pritts and
                Zuzana Kukelova and
                Viktor Larsson and
                Ondrej Chum},
  title      = {Radially-Distorted Conjugate Translations},
  journal    = {CoRR},
  volume     = {abs/1711.11339},
  year       = {2017},
  url        = {http://arxiv.org/abs/1711.11339},
  eprinttype = {arXiv},
  eprint     = {1711.11339},
  timestamp  = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1711-11339.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@manual{raspberry_pi_hq_camera_guide,
  title  = {Raspberry Pi Camera Guide},
  author = {Raspberry Pi Foundation},
  year   = {2023},
  note   = {Comprehensive guide for working with Raspberry Pi HQ Camera, detailing algorithms in the image processing pipeline. Relevant for systems using the Raspberry Pi HQ Camera and associated libraries.},
  url    = {https://datasheets.raspberrypi.com/camera/raspberry-pi-camera-guide.pdf}
}
@article{RaspberryPi_application_review_2024,
  title    = {A comprehensive review on applications of Raspberry Pi},
  journal  = {Computer Science Review},
  volume   = {52},
  pages    = {100636},
  year     = {2024},
  issn     = {1574-0137},
  doi      = {https://doi.org/10.1016/j.cosrev.2024.100636},
  url      = {https://www.sciencedirect.com/science/article/pii/S1574013724000200},
  author   = {Sudha Ellison Mathe and Hari Kishan Kondaveeti and Suseela Vappangi and Sunny Dayal Vanambathina and Nandeesh Kumar Kumaravelu},
  keywords = {Raspberry Pi, Prototyping, Automation, Monitoring, Agriculture, Healthcare, Internet of Things (IoT)},
  abstract = {Raspberry Pi is an invaluable and popular prototyping tool in scientific research for experimenting with a wide variety of ideas, ranging from simple to complex projects. This review article explores how Raspberry Pi is used in various studies, discussing its pros and cons along with its applications in various domains such as home automation, agriculture, healthcare, industrial control, and advanced research. Our aim is to provide a useful resource for researchers, educators, students, product developers, and enthusiasts, helping them to grasp the current status and discover new research possibilities using Raspberry Pi.}
}
f
@article{SangYongLee2008,
  title     = {Enhanced Autofocus Algorithm Using Robust Focus Measure and Fuzzy Reasoning},
  volume    = {18},
  issn      = {1558-2205},
  url       = {http://dx.doi.org/10.1109/TCSVT.2008.924105},
  doi       = {10.1109/tcsvt.2008.924105},
  number    = {9},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Sang-Yong Lee and Kumar,  Y. and Ji-Man Cho and Sang-Won Lee and Soo-Won Kim},
  year      = {2008},
  month     = sep,
  pages     = {1237–1246}
}

@article{Saryer2023_Microfluidics,
  title     = {Open Hardware for Microfluidics: Exploiting Raspberry Pi Singleboard Computer and Camera Systems for Customisable Laboratory Instrumentation},
  volume    = {13},
  issn      = {2079-6374},
  url       = {http://dx.doi.org/10.3390/bios13100948},
  doi       = {10.3390/bios13100948},
  number    = {10},
  journal   = {Biosensors},
  publisher = {MDPI AG},
  author    = {Sarıyer,  R\"{u}ya Meltem and Edwards,  Alexander Daniel and Needs,  Sarah Helen},
  year      = {2023},
  month     = oct,
  pages     = {948}
}

@article{SE_Calib_2023,
  author   = {Liao, Youqi and Li, Jianping and Kang, Shuhao and Li, Qiang and Zhu, Guifang and Yuan, Shenghai and Dong, Zhen and Yang, Bisheng},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {SE-Calib: Semantic Edge-Based LiDAR–Camera Boresight Online Calibration in Urban Scenes},
  year     = {2023},
  volume   = {61},
  number   = {},
  pages    = {1-13},
  keywords = {Semantics;Feature extraction;Image edge detection;Calibration;Cameras;Laser radar;Point cloud compression;Boresight parameters;mobile mapping system (MMS);semantic edge features;sensors calibration},
  doi      = {10.1109/TGRS.2023.3278024}
}
@article{SimpleAndEfficient_Zoom,
  author  = {Chen, Yong-Sheng and Shih, Sheng-Wen and Hung, Yi-Ping and Fuh, Chiou-Shann},
  year    = {2001},
  month   = {12},
  pages   = {1099-1110},
  title   = {Simple and Efficient Method of Calibrating a Motorized Zoom Lens},
  volume  = {19},
  journal = {Image and Vision Computing},
  doi     = {10.1016/S0262-8856(01)00069-5}
}
@book{smith2007modern,
  title     = {Modern Optical Engineering 4E (PB)},
  author    = {Smith, W.J.},
  isbn      = {9780071593755},
  series    = {McGraw Hill professional},
  url       = {https://books.google.com.ua/books?id=DrtM_bAnf_YC},
  year      = {2007},
  publisher = {McGraw Hill LLC}
}

@article{Subbarao1993,
  title     = {Focusing techniques},
  volume    = {32},
  issn      = {0091-3286},
  url       = {http://dx.doi.org/10.1117/12.147706},
  doi       = {10.1117/12.147706},
  number    = {11},
  journal   = {Optical Engineering},
  publisher = {SPIE-Intl Soc Optical Eng},
  author    = {Subbarao,  Murali},
  year      = {1993},
  pages     = {2824}
}

@inproceedings{Sumner2014ProcessingRI,
  title  = {Processing RAW Images in MATLAB},
  author = {RobertC Sumner},
  year   = {2014},
  url    = {https://api.semanticscholar.org/CorpusID:28078867}
}
@book{Szeliski2022-mv,
  title     = {Computer vision},
  author    = {Szeliski, Richard},
  publisher = {Springer Nature},
  series    = {Texts in computer science},
  edition   = 2,
  month     = jan,
  year      = 2022,
  address   = {Cham, Switzerland},
  language  = {en}
}
@inproceedings{Tang2017,
  title     = {Depth from Defocus in the Wild},
  url       = {http://dx.doi.org/10.1109/CVPR.2017.507},
  doi       = {10.1109/cvpr.2017.507},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Tang,  Huixuan and Cohen,  Scott and Price,  Brian and Schiller,  Stephen and Kutulakos,  Kiriakos N.},
  year      = {2017},
  month     = jul
}
@article{Tapper2012_ProblemsTsai,
  author = {Tapper, Michael and McKerrow, Phillip and Abrantes, Jo},
  year   = {2012},
  month  = {05},
  pages  = {},
  title  = {Problems encountered in the implementation of Tsai's algorithm for camera calibration}
}

Sitter, D.N., Goddard, J.S., and Ferrell, R.K., (1995), "Method for the measurement of the modulation transfer function of sampled imaging systems from bar-target patterns.", Applied Optics, v. 34 n. 4, pp. 746-751.

@inproceedings{Tarabanis,
  title     = {Modeling of a computer-controlled zoom lens},
  url       = {http://dx.doi.org/10.1109/ROBOT.1992.220032},
  doi       = {10.1109/robot.1992.220032},
  booktitle = {Proceedings 1992 IEEE International Conference on Robotics and Automation},
  publisher = {IEEE Comput. Soc. Press},
  author    = {Tarabanis,  K. and Tsai,  R.Y. and Goodman,  D.S.},
  pages     = {1545–1551}
}

@article{Tarabanis1994,
  title     = {Calibration of a Computer Controlled Robotic Vision Sensor with a Zoom Lens},
  volume    = {59},
  issn      = {1049-9660},
  url       = {http://dx.doi.org/10.1006/ciun.1994.1015},
  doi       = {10.1006/ciun.1994.1015},
  number    = {2},
  journal   = {CVGIP: Image Understanding},
  publisher = {Elsevier BV},
  author    = {Tarabanis,  K. and Tsai,  R.Y. and Goodman,  D.S.},
  year      = {1994},
  month     = mar,
  pages     = {226–241}
}







@article{universal_calib_approach_2021,
  author     = {Yaroslava Lochman and
                Kostiantyn Liepieshov and
                Jianhui Chen and
                Michal Perdoch and
                Christopher Zach and
                James Pritts},
  title      = {BabelCalib: {A} Universal Approach to Calibrating Central Cameras},
  journal    = {CoRR},
  volume     = {abs/2109.09704},
  year       = {2021},
  url        = {https://arxiv.org/abs/2109.09704},
  eprinttype = {arXiv},
  eprint     = {2109.09704},
  timestamp  = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2109-09704.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{WaveletFocus2006,
  author = {Xie, Hui and Rong, Weibin and Sun, Lining},
  year   = {2006},
  month  = {10},
  pages  = {229-234},
  title  = {Wavelet-Based Focus Measure and 3-D Surface Reconstruction Method for Microscopy Images},
  doi    = {10.1109/IROS.2006.282641}
}
@article{WEE20072533,
  title    = {Measure of image sharpness using eigenvalues},
  journal  = {Information Sciences},
  volume   = {177},
  number   = {12},
  pages    = {2533-2552},
  year     = {2007},
  issn     = {0020-0255},
  doi      = {https://doi.org/10.1016/j.ins.2006.12.023},
  url      = {https://www.sciencedirect.com/science/article/pii/S002002550700014X},
  author   = {Chong-Yaw Wee and Raveendran Paramesran},
  keywords = {Image sharpness metric, Generalized eigenvalues problem, Blur and noisy conditions, Image contrast, Covariance matrix, Singular Values Decomposition (SVD), Working range, Prediction consistency},
  abstract = {This paper deals with the design and implementation of a novel image sharpness metric based on the statistical approach. This sharpness metric is derived by modelling the image sharpness problem as a generalized eigenvalues problem. This problem is solved using Rayleigh quotient optimization where relevant statistical information of an image is extracted and then represented through a series of eigenvalues. The novelty of this paper comes from the application of eigenvalues in image sharpness metric formulation to provide robust assessment with the presence of various blur and noisy conditions. Firstly, the input image is normalized by its energy to minimize the effects caused by image contrast. Secondly, the covariance matrix is computed from this normalized image before it is diagonalized using Singular Values Decomposition (SVD) to obtain a series of eigenvalues. Finally, the image sharpness of the normalized image is determined by the trace of the first several eigenvalues. The performance of the proposed metric is gauged by comparing with several objective image sharpness metrics. Experimental results using synthetic and real images with known and unknown distortion conditions show the robustness and feasibility of the proposed metric in providing relative image sharpness. In particular, the proposed metric provides wider working range and more precise prediction consistency under all tested deformation conditions although it is slightly expensive in terms of computation than other metrics.}
}


@book{Williams2002-np,
  title     = {Introduction to optical transfer function v. {PM112}},
  author    = {Williams, Charles S and Becklund, Orville A},
  publisher = {SPIE Press},
  series    = {Press Monographs},
  month     = aug,
  year      = 2002,
  address   = {Bellingham, WA}
}
@inproceedings{Williams2003_DebunkSpecsmanship,
  author  = {Williams, Don},
  year    = {2003},
  month   = {01},
  pages   = {77-81},
  title   = {Debunking of Specsmanship: Progress on ISO/TC42 Standards for Digital Capture Imaging Performance},
  volume  = {7},
  journal = {Proc IS&T PICS Conference}
}
@inproceedings{Williams2007,
  title     = {Applying and extending ISO/TC42 digital camera resolution standards to mobile imaging products},
  volume    = {6494},
  issn      = {0277-786X},
  url       = {http://dx.doi.org/10.1117/12.703645},
  doi       = {10.1117/12.703645},
  booktitle = {Image Quality and System Performance IV},
  publisher = {SPIE},
  author    = {Williams,  Don and Burns,  Peter D.},
  year      = {2007},
  month     = jan,
  pages     = {64940H}
}


@article{Williams2009,
  title     = {Preparing for the Image Literate Decade},
  volume    = {6},
  issn      = {2161-8798},
  url       = {http://dx.doi.org/10.2352/issn.2168-3204.2009.6.1.art00026},
  doi       = {10.2352/issn.2168-3204.2009.6.1.art00026},
  number    = {1},
  journal   = {Archiving Conference},
  publisher = {Society for Imaging Science & Technology},
  author    = {Williams,  Don and Burns,  Peter D.},
  year      = {2009},
  month     = jan,
  pages     = {124–127}
}


@phdthesis{Willson1994,
  author  = {Willson, R. G.},
  title   = {Modeling and Calibration of Automated Zoom Lenses},
  school  = {Carnegie Mellon University},
  year    = {1994},
  type    = {PhD dissertation},
  address = {USA},
  note    = {UMI Order No. GAX94-1973},
  url     = {https://www.ri.cmu.edu/pub_files/pub4/willson_reg_1994_1/willson_reg_1994_1.pdf}
}
@article{Wu2021,
  title     = {Theory,  method,  and test tools for determination of 3D MTF characteristics in cone‐beam CT},
  volume    = {48},
  issn      = {2473-4209},
  url       = {http://dx.doi.org/10.1002/mp.14820},
  doi       = {10.1002/mp.14820},
  number    = {6},
  journal   = {Medical Physics},
  publisher = {Wiley},
  author    = {Wu,  Pengwei and Boone,  John M. and Hernandez,  Andrew M. and Mahesh,  Mahadevappa and Siewerdsen,  Jeffrey H.},
  year      = {2021},
  month     = apr,
  pages     = {2772–2789}
}
@article{Xia2017,
  title     = {Real‐time calibration of space zoom cameras based on fixed stars},
  volume    = {11},
  issn      = {1751-9640},
  url       = {http://dx.doi.org/10.1049/iet-cvi.2016.0145},
  doi       = {10.1049/iet-cvi.2016.0145},
  number    = {5},
  journal   = {IET Computer Vision},
  publisher = {Institution of Engineering and Technology (IET)},
  author    = {Xia,  Qi and Qi,  Naiming and Ye,  Dong and Guo,  Yubo and Wang,  Tianye},
  year      = {2017},
  month     = jun,
  pages     = {342–349}
}
@inproceedings{Yang2024OpticalMTF_Microscope,
  title     = {Optical resolution and MTF of a low-cost Fourier ptychography microscope using a raspberry pi computer},
  volume    = {2021},
  url       = {http://dx.doi.org/10.1117/12.3003269},
  doi       = {10.1117/12.3003269},
  booktitle = {Optics and Biophotonics in Low-Resource Settings X},
  publisher = {SPIE},
  author    = {Yang,  Haechan J. and Wasswa,  William and Li,  Raymond and Pautler,  Brandon and Fry,  Aidan and Leung,  Rajan and Holdsworth,  David W. and Cunningham,  Ian A.},
  editor    = {Levitz,  David and Ozcan,  Aydogan},
  year      = {2024},
  month     = mar,
  pages     = {5}
}


@article{Yuan2014,
  title     = {Laboratory geometric calibration of areal digital aerial camera},
  volume    = {17},
  issn      = {1755-1315},
  url       = {http://dx.doi.org/10.1088/1755-1315/17/1/012196},
  doi       = {10.1088/1755-1315/17/1/012196},
  journal   = {IOP Conference Series: Earth and Environmental Science},
  publisher = {IOP Publishing},
  author    = {Yuan,  F and Qi,  W J and Fang,  A P},
  year      = {2014},
  month     = mar,
  pages     = {012196}
}


@article{ZHENG201562,
  title    = {Zoom lens calibration with zoom- and focus-related intrinsic parameters applied to bundle adjustment},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume   = {102},
  pages    = {62-72},
  year     = {2015},
  issn     = {0924-2716},
  doi      = {https://doi.org/10.1016/j.isprsjprs.2015.01.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S0924271615000222},
  author   = {Shunyi Zheng and Zheng Wang and Rongyong Huang},
  keywords = {Single-lens reflex cameras, Zoom lens, Intrinsic parameter, Collinearity equation, Self-calibration, Bundle adjustment},
  abstract = {A zoom lens is more flexible for photogrammetric measurements under diverse environments than a fixed lens. However, challenges in calibration of zoom-lens cameras preclude the wide use of zoom lenses in the field of close-range photogrammetry. Thus, a novel zoom lens calibration method is proposed in this study. In this method, instead of conducting modeling after monofocal calibrations, we summarize the empirical zoom/focus models of intrinsic parameters first and then incorporate these parameters into traditional collinearity equations to construct the fundamental mathematical model, i.e., collinearity equations with zoom- and focus-related intrinsic parameters. Similar to monofocal calibration, images taken at several combinations of zoom and focus settings are processed in a single self-calibration bundle adjustment. In the self-calibration bundle adjustment, three types of unknowns, namely, exterior orientation parameters, unknown space point coordinates, and model coefficients of the intrinsic parameters, are solved simultaneously. Experiments on three different digital cameras with zoom lenses support the feasibility of the proposed method, and their relative accuracies range from 1:4000 to 1:15,100. Furthermore, the nominal focal length written in the exchangeable image file header is found to lack reliability in experiments. Thereafter, the joint influence of zoom lens instability and zoom recording errors is further analyzed quantitatively. The analysis result is consistent with the experimental result and explains the reason why zoom lens calibration can never have the same accuracy as monofocal self-calibration.}
}


@article{ZoomDependantCamCalib_Photogrammetry_2006,
  author  = {Fraser, Clive and Al-Ajlouni, S.},
  year    = {2006},
  month   = {09},
  pages   = {1017-1026},
  title   = {Zoom-Dependent Camera Calibration in Digital Close-Range Photogrammetry},
  volume  = {72},
  journal = {Photogrammetric Engineering & Remote Sensing},
  doi     = {10.14358/PERS.72.9.1017}
}
